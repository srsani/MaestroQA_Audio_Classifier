{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Four model test with twitter_glove200\n",
    "\n",
    "Data set:\n",
    "1.\tText data from the audio calls.\n",
    "\n",
    "2. Dataset from Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis (WASSA-2017), which is to be held in conjunction with EMNLP-2017, which can be downloaded [here](http://saifmohammad.com/WebPages/EmotionIntensity-SharedTask.html)\n",
    "\n",
    "Preprinted Glove:\n",
    "-\tCommon Crawl 840B tokens, 2.2M vocab, cased, 300d vectors, 2.03 GB download): glove.840B.300d.zip, which can be downloaded [here](https://nlp.stanford.edu/projects/glove/). \n",
    "\n",
    "I have tested the performance of the following models:\n",
    "\n",
    "1. Model1: LSTM (0.6120)\n",
    "2. Model2: Bidirectional GRU(0.6279)\n",
    "3. Model3: Single Convolutional Neural Network (0.5176)\n",
    "4. Model4: Multi-size CNN (0.6005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys, os, re, csv, codecs \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D\n",
    "from keras.models import Model\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from keras.layers import Input, Dense, Embedding, SpatialDropout1D, concatenate\n",
    "from keras.layers import GRU, Bidirectional, GlobalAveragePooling1D, GlobalMaxPooling1D\n",
    "from glob import glob\n",
    "from keras.layers.convolutional import Conv1D, MaxPooling1D\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Merge\n",
    "from keras.layers.convolutional import Conv1D, MaxPooling1D\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.models import model_from_json\n",
    "import pickle\n",
    "from sklearn.utils import resample\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from multiprocessing import Pool\n",
    "\n",
    "stemming = PorterStemmer()\n",
    "from nltk.stem import SnowballStemmer\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "\n",
    "def stowords(words):\n",
    "    words = str(words)\n",
    "    words = words.lower()\n",
    "    words = words.replace(\"what's\", \"what is \")\n",
    "    words = words.replace(\"shouldn't\", \"should not \")\n",
    "    words= words.replace(\"\\'ve\", \" have \")\n",
    "    words= words.replace(\"'s\", \" \")\n",
    "    words= words.replace(\"can't\", \"cannot \")\n",
    "    words= words.replace(\"don't\", \"do not \")\n",
    "    words= words.replace(\"doesn't\", \"does not \")\n",
    "    words= words.replace(\"n't\", \"not \")\n",
    "    words = words.replace(r\"i'm\", \"i am\")\n",
    "    words =words.replace(r\" m \", \" am \")\n",
    "    words =words.replace(r\"\\'re\", \" are \")\n",
    "    words =words.replace(r\"\\'d\", \" would \")\n",
    "    words =words.replace(r\"'ll\", \" will \") \n",
    "    \n",
    "    words = words.replace(\"'\", \"\")\n",
    "    words= words.replace(\"!\", \"\") \n",
    "    words =words.replace(r\"´\", \"\")\n",
    "    words = words.replace(\"`\", \"\")\n",
    "    words = words.replace(\"\\r\\n\", \" \")\n",
    "#     words = \" \".join([stemmer.stem(word) for word in words.split()])\n",
    "    \n",
    "#     words = \" \".join([stemming.stem(word) for word in words.split()])\n",
    "#     words = \" \".join([Word(word).lemmatize()for word in words.split()])\n",
    "    \n",
    "   \n",
    "    letters_only = re.sub(\"[^a-zA-Zé]\", \" \", words)\n",
    "    word = letters_only.lower().split()\n",
    " \n",
    "    se = set(STOP_WORDS)\n",
    "    meaningful_words =\" \".join( [w for w in word if w not in se]) \n",
    "#     meaningful_words = str(TextBlob(meaningful_words).correct())\n",
    "    \n",
    "    return ( meaningful_words )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_hdf('./Data/dataset2_tags_texts.hdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5670, 217)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gradable_id</th>\n",
       "      <th>polarity</th>\n",
       "      <th>subjectivity</th>\n",
       "      <th>text</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>mfcc_1_m1_bin1</th>\n",
       "      <th>mfcc_2_m1_bin1</th>\n",
       "      <th>mfcc_3_m1_bin1</th>\n",
       "      <th>mfcc_4_m1_bin1</th>\n",
       "      <th>mfcc_1_m2_bin1</th>\n",
       "      <th>...</th>\n",
       "      <th>mfcc_feat_var_1</th>\n",
       "      <th>mfcc_feat_var_2</th>\n",
       "      <th>mfcc_feat_var_3</th>\n",
       "      <th>mfcc_feat_var_4</th>\n",
       "      <th>qa_score</th>\n",
       "      <th>tags</th>\n",
       "      <th>time</th>\n",
       "      <th>clean_tag</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>text_lenght</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>418139</td>\n",
       "      <td>0.318571</td>\n",
       "      <td>0.500516</td>\n",
       "      <td>['(0.9353249073028564): hi thank you for calli...</td>\n",
       "      <td>922</td>\n",
       "      <td>8.075193</td>\n",
       "      <td>21.911402</td>\n",
       "      <td>17.653523</td>\n",
       "      <td>10.752469</td>\n",
       "      <td>18.578279</td>\n",
       "      <td>...</td>\n",
       "      <td>25.014487</td>\n",
       "      <td>193.926085</td>\n",
       "      <td>131.714832</td>\n",
       "      <td>212.163095</td>\n",
       "      <td>38</td>\n",
       "      <td>[\"shipping\", \"tracking_inquiry__processing\"]</td>\n",
       "      <td>2017-03-13T15:43:02</td>\n",
       "      <td>shipping tracking inquiry processing</td>\n",
       "      <td>hi thank you for calling fabfitfun meetings le...</td>\n",
       "      <td>1345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1433069</td>\n",
       "      <td>0.262808</td>\n",
       "      <td>0.417980</td>\n",
       "      <td>['(0.7501416802406311): thanks again he resear...</td>\n",
       "      <td>4476</td>\n",
       "      <td>6.030269</td>\n",
       "      <td>22.738942</td>\n",
       "      <td>17.901995</td>\n",
       "      <td>12.519905</td>\n",
       "      <td>18.912234</td>\n",
       "      <td>...</td>\n",
       "      <td>21.230926</td>\n",
       "      <td>163.387397</td>\n",
       "      <td>124.742727</td>\n",
       "      <td>210.699407</td>\n",
       "      <td>93</td>\n",
       "      <td>[\"order__shipped__order_status\"]</td>\n",
       "      <td>2017-05-08T14:51:16</td>\n",
       "      <td>order shipped order status</td>\n",
       "      <td>thanks again he researches chelsea how may hel...</td>\n",
       "      <td>1875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18834</td>\n",
       "      <td>0.234722</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>['(0.9273091554641724): thank you for calling ...</td>\n",
       "      <td>62</td>\n",
       "      <td>6.775557</td>\n",
       "      <td>22.189641</td>\n",
       "      <td>17.218559</td>\n",
       "      <td>12.555730</td>\n",
       "      <td>18.399315</td>\n",
       "      <td>...</td>\n",
       "      <td>26.413125</td>\n",
       "      <td>267.558405</td>\n",
       "      <td>96.075137</td>\n",
       "      <td>191.671796</td>\n",
       "      <td>100</td>\n",
       "      <td>[\"application\", \"application_status\", \"non_sta...</td>\n",
       "      <td>2017-05-11T14:21:27</td>\n",
       "      <td>application application status non state employee</td>\n",
       "      <td>thank you for calling the state of tennessee d...</td>\n",
       "      <td>828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1498618</td>\n",
       "      <td>0.345833</td>\n",
       "      <td>0.462500</td>\n",
       "      <td>[\"(0.9570504426956177): thank you for calling ...</td>\n",
       "      <td>4850</td>\n",
       "      <td>5.875857</td>\n",
       "      <td>23.094995</td>\n",
       "      <td>16.586156</td>\n",
       "      <td>14.985884</td>\n",
       "      <td>17.504581</td>\n",
       "      <td>...</td>\n",
       "      <td>12.566649</td>\n",
       "      <td>142.067990</td>\n",
       "      <td>101.407129</td>\n",
       "      <td>145.302891</td>\n",
       "      <td>84</td>\n",
       "      <td>[\"shave_plans__cancel_plan__overstocked\"]</td>\n",
       "      <td>2017-06-20T20:34:38</td>\n",
       "      <td>shave plans cancel plan overstocked</td>\n",
       "      <td>thank you for calling harry s my name is kayle...</td>\n",
       "      <td>736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1446730</td>\n",
       "      <td>0.087349</td>\n",
       "      <td>0.440519</td>\n",
       "      <td>[\"(0.9487425088882446): hi yes this is Zack fr...</td>\n",
       "      <td>4555</td>\n",
       "      <td>5.762013</td>\n",
       "      <td>22.092987</td>\n",
       "      <td>16.735214</td>\n",
       "      <td>20.478581</td>\n",
       "      <td>18.337259</td>\n",
       "      <td>...</td>\n",
       "      <td>18.309896</td>\n",
       "      <td>181.549078</td>\n",
       "      <td>105.479915</td>\n",
       "      <td>184.641216</td>\n",
       "      <td>75</td>\n",
       "      <td>[\"callback\", \"site__profile__edit_email\"]</td>\n",
       "      <td>2017-05-17T19:08:16</td>\n",
       "      <td>callback site profile edit email</td>\n",
       "      <td>hi yes this is zack from harry s giving you a ...</td>\n",
       "      <td>1886</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 217 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   gradable_id  polarity  subjectivity  \\\n",
       "0       418139  0.318571      0.500516   \n",
       "1      1433069  0.262808      0.417980   \n",
       "2        18834  0.234722      0.375000   \n",
       "3      1498618  0.345833      0.462500   \n",
       "4      1446730  0.087349      0.440519   \n",
       "\n",
       "                                                text  Unnamed: 0  \\\n",
       "0  ['(0.9353249073028564): hi thank you for calli...         922   \n",
       "1  ['(0.7501416802406311): thanks again he resear...        4476   \n",
       "2  ['(0.9273091554641724): thank you for calling ...          62   \n",
       "3  [\"(0.9570504426956177): thank you for calling ...        4850   \n",
       "4  [\"(0.9487425088882446): hi yes this is Zack fr...        4555   \n",
       "\n",
       "   mfcc_1_m1_bin1  mfcc_2_m1_bin1  mfcc_3_m1_bin1  mfcc_4_m1_bin1  \\\n",
       "0        8.075193       21.911402       17.653523       10.752469   \n",
       "1        6.030269       22.738942       17.901995       12.519905   \n",
       "2        6.775557       22.189641       17.218559       12.555730   \n",
       "3        5.875857       23.094995       16.586156       14.985884   \n",
       "4        5.762013       22.092987       16.735214       20.478581   \n",
       "\n",
       "   mfcc_1_m2_bin1     ...       mfcc_feat_var_1  mfcc_feat_var_2  \\\n",
       "0       18.578279     ...             25.014487       193.926085   \n",
       "1       18.912234     ...             21.230926       163.387397   \n",
       "2       18.399315     ...             26.413125       267.558405   \n",
       "3       17.504581     ...             12.566649       142.067990   \n",
       "4       18.337259     ...             18.309896       181.549078   \n",
       "\n",
       "   mfcc_feat_var_3  mfcc_feat_var_4  qa_score  \\\n",
       "0       131.714832       212.163095        38   \n",
       "1       124.742727       210.699407        93   \n",
       "2        96.075137       191.671796       100   \n",
       "3       101.407129       145.302891        84   \n",
       "4       105.479915       184.641216        75   \n",
       "\n",
       "                                                tags                 time  \\\n",
       "0       [\"shipping\", \"tracking_inquiry__processing\"]  2017-03-13T15:43:02   \n",
       "1                   [\"order__shipped__order_status\"]  2017-05-08T14:51:16   \n",
       "2  [\"application\", \"application_status\", \"non_sta...  2017-05-11T14:21:27   \n",
       "3          [\"shave_plans__cancel_plan__overstocked\"]  2017-06-20T20:34:38   \n",
       "4          [\"callback\", \"site__profile__edit_email\"]  2017-05-17T19:08:16   \n",
       "\n",
       "                                           clean_tag  \\\n",
       "0               shipping tracking inquiry processing   \n",
       "1                         order shipped order status   \n",
       "2  application application status non state employee   \n",
       "3                shave plans cancel plan overstocked   \n",
       "4                   callback site profile edit email   \n",
       "\n",
       "                                          clean_text  text_lenght  \n",
       "0  hi thank you for calling fabfitfun meetings le...         1345  \n",
       "1  thanks again he researches chelsea how may hel...         1875  \n",
       "2  thank you for calling the state of tennessee d...          828  \n",
       "3  thank you for calling harry s my name is kayle...          736  \n",
       "4  hi yes this is zack from harry s giving you a ...         1886  \n",
       "\n",
       "[5 rows x 217 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cat(x):\n",
    "    if x == 0:\n",
    "        return ('bad')\n",
    "    else:\n",
    "        return ('good')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['cat'] = df.category.apply(cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = df.join(pd.get_dummies(df.cat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['gradable_id', 'polarity', 'subjectivity', 'text', 'Unnamed: 0',\n",
       "       'mfcc_1_m1_bin1', 'mfcc_2_m1_bin1', 'mfcc_3_m1_bin1', 'mfcc_4_m1_bin1',\n",
       "       'mfcc_1_m2_bin1',\n",
       "       ...\n",
       "       'mfcc_feat_var_4', 'qa_score', 'tags', 'time', 'clean_tag',\n",
       "       'clean_text', 'text_lenght', 'cat', 'bad', 'good'],\n",
       "      dtype='object', length=220)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pool = Pool(processes = 28) # or some number of your choice\n",
    "df['cleaner_text'] = pool.map(stowords, df.clean_text)\n",
    "pool.terminate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EMBEDDING_FILE = '/home/sohrab/streetbees/emotion_detection/crawl-300d-2M.vec'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading GloVe vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_coefs(word, *arr): return word, np.asarray(arr, dtype='float32')\n",
    "embeddings_index = dict(get_coefs(*o.rstrip().rsplit(' ')) for o in open(EMBEDDING_FILE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "570616"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This model has vectors for 1193514 words \n",
    "len(embeddings_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embeddings_index['what'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embed_size = 300 # how big is each word vector\n",
    "max_features = 45000 # how many unique words to use (this can be a hyperparameter)\n",
    "maxlen = 50 # max number of words in a comment to use (this can be a hyperparameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = df[['cleaner_text','bad','good']].copy().reindex()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Tokenization: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# train\n",
    "list_sentences_train = train[\"cleaner_text\"].fillna(\"_na_\").values\n",
    "y = train[['bad','good']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "\n",
    "tokenizer.fit_on_texts(list(list_sentences_train))\n",
    "\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\n",
    "\n",
    "X_t = pad_sequences(list_tokenized_train, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'thanks researches chelsea help n hi chelsea richard griffith calling check sure order went double check price shipment sorry returning customer harry s quit come tried place order blaze guess butter shaving cream wanted sure went information correct check s n auburn tumbler gmail com n okay great let s n francia found account looks like email s gmx instead gmail email think s verify shipping address sure right account street montgomery alabama n yep found account think good order shipped sea n okay order process yesterday looks like haven t updated tracking information sent n anti leaves sent th n free trials heading way n okay set blades shipped th th right trials heading way n okay th email telling today shipped wait invoice order goes email days shipment email day ship reminder ll send email problem look forward great day thanks calling n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.cleaner_text[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   1, 9297, 2093,   70,   39,    1,   66, 2188, 1701,   51,    1,\n",
       "          3,   61,   60,  120,   39,   39,   18, 2188, 1701,   51,    1,\n",
       "          3,   39,    9,  622,   19,  120,  245,  624,   22,  355,    9,\n",
       "         54,   91,    9,   27,  124,  597,   13,   41,    9,   46,   47,\n",
       "        277,   43,   27,   63,   17,    1], dtype=int32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_t[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index\n",
    "nb_words = min(max_features, len(word_index))\n",
    "embedding_matrix = np.zeros((max_features, embed_size))\n",
    "\n",
    "missing_words = []\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_features: continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is None:\n",
    "        missing_words.append(word)\n",
    "    elif embedding_vector is not None: \n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embedding_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23607"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#total number of words in our dataset:\n",
    "len(word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inp = Input(shape=(maxlen,))\n",
    "x = Embedding(max_features, embed_size, weights=[embedding_matrix])(inp)\n",
    "x = Bidirectional(LSTM(50, return_sequences=True, dropout=0.1, recurrent_dropout=0.1))(x)\n",
    "x = GlobalMaxPool1D()(x)\n",
    "x = Dense(50, activation=\"relu\")(x)\n",
    "x = Dropout(0.1)(x)\n",
    "x = Dense(2, activation=\"sigmoid\")(x)\n",
    "model = Model(inputs=inp, outputs=x)\n",
    "optim = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, decay=0.001)\n",
    "model.compile(loss='binary_crossentropy', optimizer=optim, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5103 samples, validate on 567 samples\n",
      "Epoch 1/2\n",
      "5103/5103 [==============================] - 10s 2ms/step - loss: 0.6773 - acc: 0.5690 - val_loss: 0.6550 - val_acc: 0.5926\n",
      "Epoch 2/2\n",
      "5103/5103 [==============================] - 9s 2ms/step - loss: 0.6255 - acc: 0.6356 - val_loss: 0.6389 - val_acc: 0.6120\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_t, y, batch_size=128, epochs=2, validation_split=0.1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inp = Input(shape=(maxlen, ))\n",
    "x = Embedding(max_features, embed_size, weights=[embedding_matrix])(inp)\n",
    "x = SpatialDropout1D(0.2)(x)\n",
    "x = Bidirectional(GRU(70, return_sequences=True))(x)\n",
    "avg_pool = GlobalAveragePooling1D()(x)\n",
    "max_pool = GlobalMaxPooling1D()(x)\n",
    "conc = concatenate([avg_pool, max_pool])\n",
    "outp = Dense(2, activation=\"sigmoid\")(conc)\n",
    "lr = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, decay=0.001)   \n",
    "model = Model(inputs=inp, outputs=outp)\n",
    "model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=lr,\n",
    "                  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5103 samples, validate on 567 samples\n",
      "Epoch 1/3\n",
      "5103/5103 [==============================] - 18s 3ms/step - loss: 0.6689 - acc: 0.5827 - val_loss: 0.6475 - val_acc: 0.5917\n",
      "Epoch 2/3\n",
      "5103/5103 [==============================] - 15s 3ms/step - loss: 0.6135 - acc: 0.6515 - val_loss: 0.6268 - val_acc: 0.6296\n",
      "Epoch 3/3\n",
      "5103/5103 [==============================] - 16s 3ms/step - loss: 0.5493 - acc: 0.7177 - val_loss: 0.6184 - val_acc: 0.6279\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_t, y, \n",
    "          batch_size=16, \n",
    "          epochs=3, \n",
    "          validation_split=0.1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 3:\n",
    "\n",
    "### Single convolutional neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inp = Input(shape=(maxlen, ))\n",
    "x = Embedding(max_features, embed_size, weights=[embedding_matrix])(inp)\n",
    "x = Dropout(0.6)(x)\n",
    "x = Conv1D(64, 5, padding='same', activation='relu')(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = MaxPooling1D()(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(100, activation='relu')(x)\n",
    "x = Dropout(0.6)(x)\n",
    "x = Dense(2, activation='sigmoid')(x)\n",
    "\n",
    "model = Model(inputs=inp, outputs=x)\n",
    "optim = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, decay=0.001)\n",
    "model.compile(loss='binary_crossentropy', \n",
    "              optimizer=optim, \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5103 samples, validate on 567 samples\n",
      "Epoch 1/3\n",
      "5103/5103 [==============================] - 2s 438us/step - loss: 0.7060 - acc: 0.5012 - val_loss: 0.6930 - val_acc: 0.5159\n",
      "Epoch 2/3\n",
      "5103/5103 [==============================] - 1s 218us/step - loss: 0.6924 - acc: 0.5129 - val_loss: 0.6924 - val_acc: 0.5088\n",
      "Epoch 3/3\n",
      "5103/5103 [==============================] - 1s 221us/step - loss: 0.6872 - acc: 0.5414 - val_loss: 0.6871 - val_acc: 0.5176\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_t, y, batch_size=16, epochs=3, validation_split=0.1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 4:\n",
    "\n",
    "### Multi-size CNN:\n",
    "\n",
    "This is an implementation of a multi-size CNN as shown in Ben Bowle's excellent blog [post](https://quid.com/feed/how-quid-uses-deep-learning-with-small-data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sohrab/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:8: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n"
     ]
    }
   ],
   "source": [
    "graph_in = Input ((maxlen, 300))\n",
    "convs = [ ] \n",
    "for fsz in range (3, 6): \n",
    "    x = Conv1D(64, fsz, padding='same', activation='relu')(graph_in)\n",
    "    x = MaxPooling1D()(x) \n",
    "    x = Flatten()(x) \n",
    "    convs.append(x)\n",
    "out = Merge(mode=\"concat\")(convs) \n",
    "graph = Model(graph_in, out) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inp = Input(shape=(maxlen,))\n",
    "x = Embedding(max_features, embed_size, weights=[embedding_matrix])(inp)\n",
    "x = graph(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = Dense(100, activation='relu')(x)\n",
    "x = Dropout(0.6)(x)\n",
    "x = Dense(2, activation='sigmoid')(x)\n",
    "\n",
    "model = Model(inputs=inp, outputs=x)\n",
    "optim = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, decay=0.001)\n",
    "model.compile(loss='binary_crossentropy', \n",
    "              optimizer=optim, \n",
    "              metrics=['accuracy'],\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5103 samples, validate on 567 samples\n",
      "Epoch 1/2\n",
      "5103/5103 [==============================] - 5s 978us/step - loss: 0.5510 - acc: 0.7173 - val_loss: 0.6330 - val_acc: 0.6058\n",
      "Epoch 2/2\n",
      "5103/5103 [==============================] - 5s 977us/step - loss: 0.3096 - acc: 0.8790 - val_loss: 0.7871 - val_acc: 0.6005\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_t, y, \n",
    "          batch_size=16, \n",
    "          epochs=2, \n",
    "          validation_split=0.1,\n",
    "         );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
