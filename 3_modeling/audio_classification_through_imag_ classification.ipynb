{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audio classification through image classification:\n",
    "\n",
    "This notebook shows how to use a pre-trained resent with additional Fully Convolutional Net to predict which call is good or bad. \n",
    "\n",
    "Advantage:\n",
    "\n",
    "- The advantage of this method is that it does not require any audio file feature engineering and nor it relies on the text that is associated with each audio recording. This means it can be faster and much cheaper.\n",
    "\n",
    "Disadvantage: \n",
    "\n",
    "- The accuracy on the current dataset is around 60% which is lower than both the MFCC and NLP based approaches. But as will be shown in the following this model, and many other models that I tested, start overfitting as the training process continues which indicates that more data will improves the accuracy of this model.   \n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current folder:  /home/sohrab/MaestroQA_ffast_ai\n",
      "model path:  /home/sohrab/MaestroQA_ffast_ai/Data/crop_data/models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cuDNN version 6021 on context None\n",
      "Mapped name None to device cuda: Quadro P6000 (0000:03:00.0)\n",
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "# Rather than importing everything manually, we'll make things easy\n",
    "#   and load them all in utils.py, and just import them from there.\n",
    "%matplotlib inline\n",
    "import utils; \n",
    "\n",
    "import importlib\n",
    "importlib.reload(utils)\n",
    "\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "def save_array(fname, arr): \n",
    "    c=bcolz.carray(arr, rootdir=fname, mode='w'); c.flush()\n",
    "def load_array(fname):\n",
    "    return bcolz.open(fname)[:]\n",
    "\n",
    "from utils import *\n",
    "current_dir = os.getcwd()\n",
    "print ('current folder: ',current_dir)\n",
    "model_path = current_dir+'/Data/crop_data/models'\n",
    "print ('model path: ',model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import resnet50\n",
    "from resnet50 import Resnet50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the victories images :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10326, 3, 430, 1246) (1999, 3, 430, 1246) (10326, 2) (1999, 2)\n"
     ]
    }
   ],
   "source": [
    "trn_data_1 = load_array(current_dir+'/Data/crop_data/models/train_data_big.bc')\n",
    "val_data_1 = load_array(current_dir+'/Data/crop_data/models/valid_data_big.bc')\n",
    "\n",
    "trn_labels_1 = load_array(current_dir+'/Data/crop_data/models/trn_labels_big.bc')\n",
    "val_labels_1 = load_array(current_dir+'/Data/crop_data/models/val_labels_big.bc')\n",
    "\n",
    "print (trn_data_1.shape,val_data_1.shape, trn_labels_1.shape, val_labels_1.shape )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the Resnet50 model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sohrab/anaconda3/lib/python3.5/site-packages/keras/layers/core.py:630: UserWarning: `output_shape` argument not specified for layer lambda_1 and cannot be automatically inferred with the Theano backend. Defaulting to output shape `(None, 3, 430, 1246)` (same as input shape). If the expected output shape is different, specify it via the `output_shape` argument.\n",
      "  .format(self.name, input_shape))\n",
      "/home/sohrab/MaestroQA_ffast_ai/resnet50.py:61: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (7, 7), name=\"conv1\", strides=(2, 2))`\n",
      "  x = Convolution2D(64, 7, 7, subsample=(2, 2), name='conv1')(x)\n"
     ]
    }
   ],
   "source": [
    "rn0 = Resnet50(include_top=False , size= (430,1246)).model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10326/10326 [==============================] - 397s 38ms/step\n",
      "1999/1999 [==============================] - 77s 38ms/step\n"
     ]
    }
   ],
   "source": [
    "# Extracting feature from the pre-trained model\n",
    "trn_features = rn0.predict(trn_data_1, batch_size=128, verbose=1)\n",
    "val_features = rn0.predict(val_data_1, batch_size=128, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding the Fully Convolutional Net:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nf=128; p=0\n",
    "def get_lrg_layers():\n",
    "    return [\n",
    "        BatchNormalization(axis=1, input_shape=(2048, 14, 39)),\n",
    "        Convolution2D(nf,3,3, activation='relu', border_mode='same'),\n",
    "        BatchNormalization(axis=1),\n",
    "#         MaxPooling2D(),\n",
    "        Convolution2D(nf,3,3, activation='relu', border_mode='same'),\n",
    "        BatchNormalization(axis=1),\n",
    "#         MaxPooling2D(),\n",
    "        Convolution2D(nf,3,3, activation='relu', border_mode='same'),\n",
    "        BatchNormalization(axis=1),\n",
    "#         MaxPooling2D((1,1)),\n",
    "        Convolution2D(2,3,3, border_mode='same'),\n",
    "        Dropout(p),\n",
    "        GlobalAveragePooling2D(),\n",
    "        Activation('softmax')\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sohrab/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:5: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), padding=\"same\", activation=\"relu\")`\n",
      "/home/sohrab/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:8: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), padding=\"same\", activation=\"relu\")`\n",
      "/home/sohrab/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:11: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), padding=\"same\", activation=\"relu\")`\n",
      "/home/sohrab/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:14: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(2, (3, 3), padding=\"same\")`\n"
     ]
    }
   ],
   "source": [
    "lrg_model = Sequential(get_lrg_layers())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lrg_model.compile(Adam(lr=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10326 samples, validate on 1999 samples\n",
      "Epoch 1/3\n",
      "10326/10326 [==============================] - 36s 4ms/step - loss: 0.6990 - acc: 0.5530 - val_loss: 0.7061 - val_acc: 0.5508\n",
      "Epoch 2/3\n",
      "10326/10326 [==============================] - 37s 4ms/step - loss: 0.6767 - acc: 0.5809 - val_loss: 0.6908 - val_acc: 0.5448\n",
      "Epoch 3/3\n",
      "10326/10326 [==============================] - 37s 4ms/step - loss: 0.6627 - acc: 0.6011 - val_loss: 0.6843 - val_acc: 0.5548\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7eeb8b185908>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size=32\n",
    "lrg_model.fit(trn_features, trn_labels_1, epochs=3, batch_size=batch_size, \n",
    "       validation_data=(val_features, val_labels_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10326 samples, validate on 1999 samples\n",
      "Epoch 1/3\n",
      "10326/10326 [==============================] - 39s 4ms/step - loss: 0.6626 - acc: 0.5976 - val_loss: 0.6737 - val_acc: 0.5808\n",
      "Epoch 2/3\n",
      "10326/10326 [==============================] - 39s 4ms/step - loss: 0.6465 - acc: 0.6195 - val_loss: 0.6940 - val_acc: 0.5683\n",
      "Epoch 3/3\n",
      "10326/10326 [==============================] - 40s 4ms/step - loss: 0.6297 - acc: 0.6365 - val_loss: 0.6980 - val_acc: 0.5753\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7eeb928bc898>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size=8\n",
    "lrg_model.fit(trn_features, trn_labels_1, epochs=1, batch_size=batch_size, \n",
    "       validation_data=(val_features, val_labels_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10326 samples, validate on 1999 samples\n",
      "Epoch 1/5\n",
      "10326/10326 [==============================] - 40s 4ms/step - loss: 0.6088 - acc: 0.6611 - val_loss: 0.6861 - val_acc: 0.5808\n",
      "Epoch 2/5\n",
      "10326/10326 [==============================] - 40s 4ms/step - loss: 0.5830 - acc: 0.6840 - val_loss: 0.7114 - val_acc: 0.5888\n",
      "Epoch 3/5\n",
      "10326/10326 [==============================] - 40s 4ms/step - loss: 0.5442 - acc: 0.7190 - val_loss: 0.7815 - val_acc: 0.5493\n",
      "Epoch 4/5\n",
      "10326/10326 [==============================] - 40s 4ms/step - loss: 0.4975 - acc: 0.7514 - val_loss: 0.7881 - val_acc: 0.5763\n",
      "Epoch 5/5\n",
      "10326/10326 [==============================] - 40s 4ms/step - loss: 0.4428 - acc: 0.7847 - val_loss: 0.8099 - val_acc: 0.5553\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7eeb8af27c18>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size=16\n",
    "lrg_model.fit(trn_features, trn_labels_1, epochs=3, batch_size=batch_size, \n",
    "       validation_data=(val_features, val_labels_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10326 samples, validate on 1999 samples\n",
      "Epoch 1/5\n",
      "10326/10326 [==============================] - 44s 4ms/step - loss: 0.4740 - acc: 0.7689 - val_loss: 0.8702 - val_acc: 0.5538\n",
      "Epoch 2/5\n",
      "10326/10326 [==============================] - 44s 4ms/step - loss: 0.4101 - acc: 0.8067 - val_loss: 0.9432 - val_acc: 0.5693\n",
      "Epoch 3/5\n",
      "10326/10326 [==============================] - 43s 4ms/step - loss: 0.3606 - acc: 0.8374 - val_loss: 0.9643 - val_acc: 0.5703\n",
      "Epoch 4/5\n",
      "10326/10326 [==============================] - 44s 4ms/step - loss: 0.3131 - acc: 0.8611 - val_loss: 1.1057 - val_acc: 0.5503\n",
      "Epoch 5/5\n",
      "10326/10326 [==============================] - 43s 4ms/step - loss: 0.2783 - acc: 0.8790 - val_loss: 1.0925 - val_acc: 0.5728\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7eeb97a81048>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size=8\n",
    "lrg_model.fit(trn_features, trn_labels_1, epochs=5, batch_size=batch_size, \n",
    "       validation_data=(val_features, val_labels_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nf=128; p=0.2\n",
    "def get_lrg_layers():\n",
    "    return [\n",
    "        BatchNormalization(axis=1, input_shape=(2048, 14, 39)),\n",
    "        Convolution2D(nf,3,3, activation='relu', border_mode='same'),\n",
    "        BatchNormalization(axis=1),\n",
    "         MaxPooling2D(),\n",
    "        Convolution2D(nf,3,3, activation='relu', border_mode='same'),\n",
    "        BatchNormalization(axis=1),\n",
    "         MaxPooling2D(),\n",
    "        Convolution2D(nf,3,3, activation='relu', border_mode='same'),\n",
    "        BatchNormalization(axis=1),\n",
    "         MaxPooling2D((1,1)),\n",
    "        Convolution2D(2,3,3, border_mode='same'),\n",
    "        Dropout(p),\n",
    "        GlobalAveragePooling2D(),\n",
    "        Activation('softmax')\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sohrab/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:5: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), padding=\"same\", activation=\"relu\")`\n",
      "/home/sohrab/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:8: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), padding=\"same\", activation=\"relu\")`\n",
      "/home/sohrab/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:11: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), padding=\"same\", activation=\"relu\")`\n",
      "/home/sohrab/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:14: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(2, (3, 3), padding=\"same\")`\n"
     ]
    }
   ],
   "source": [
    "lrg_model = Sequential(get_lrg_layers())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lrg_model.compile(Adam(lr=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10326 samples, validate on 1999 samples\n",
      "Epoch 1/3\n",
      "10326/10326 [==============================] - 34s 3ms/step - loss: 0.7090 - acc: 0.5335 - val_loss: 0.7061 - val_acc: 0.5073\n",
      "Epoch 2/3\n",
      "10326/10326 [==============================] - 34s 3ms/step - loss: 0.6775 - acc: 0.5846 - val_loss: 0.6971 - val_acc: 0.5448\n",
      "Epoch 3/3\n",
      "10326/10326 [==============================] - 35s 3ms/step - loss: 0.6553 - acc: 0.6096 - val_loss: 0.6933 - val_acc: 0.5668\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7eeb85ef4278>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size=32\n",
    "lrg_model.fit(trn_features, trn_labels_1, epochs=3, batch_size=batch_size, \n",
    "       validation_data=(val_features, val_labels_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10326 samples, validate on 1999 samples\n",
      "Epoch 1/20\n",
      "10326/10326 [==============================] - 41s 4ms/step - loss: 0.6765 - acc: 0.5792 - val_loss: 0.6888 - val_acc: 0.5388\n",
      "Epoch 2/20\n",
      "10326/10326 [==============================] - 39s 4ms/step - loss: 0.6503 - acc: 0.6155 - val_loss: 0.7132 - val_acc: 0.5563\n",
      "Epoch 3/20\n",
      "10326/10326 [==============================] - 40s 4ms/step - loss: 0.6239 - acc: 0.6519 - val_loss: 0.7051 - val_acc: 0.5638\n",
      "Epoch 4/20\n",
      "10326/10326 [==============================] - 39s 4ms/step - loss: 0.5710 - acc: 0.6905 - val_loss: 0.7303 - val_acc: 0.5643\n",
      "Epoch 5/20\n",
      "10326/10326 [==============================] - 40s 4ms/step - loss: 0.4988 - acc: 0.7508 - val_loss: 1.1591 - val_acc: 0.5198\n",
      "Epoch 6/20\n",
      "10326/10326 [==============================] - 39s 4ms/step - loss: 0.3971 - acc: 0.8190 - val_loss: 1.0384 - val_acc: 0.5518\n",
      "Epoch 7/20\n",
      "10326/10326 [==============================] - 40s 4ms/step - loss: 0.3037 - acc: 0.8657 - val_loss: 0.9946 - val_acc: 0.5733\n",
      "Epoch 8/20\n",
      "10326/10326 [==============================] - 39s 4ms/step - loss: 0.2377 - acc: 0.9026 - val_loss: 1.1859 - val_acc: 0.5563\n",
      "Epoch 9/20\n",
      "10326/10326 [==============================] - 41s 4ms/step - loss: 0.1850 - acc: 0.9230 - val_loss: 1.3486 - val_acc: 0.5423\n",
      "Epoch 10/20\n",
      "10326/10326 [==============================] - 39s 4ms/step - loss: 0.1527 - acc: 0.9344 - val_loss: 1.7015 - val_acc: 0.5343\n",
      "Epoch 11/20\n",
      "10326/10326 [==============================] - 40s 4ms/step - loss: 0.1289 - acc: 0.9488 - val_loss: 1.5507 - val_acc: 0.5628\n",
      "Epoch 12/20\n",
      "10326/10326 [==============================] - 39s 4ms/step - loss: 0.1213 - acc: 0.9524 - val_loss: 1.6116 - val_acc: 0.5523\n",
      "Epoch 13/20\n",
      "10326/10326 [==============================] - 41s 4ms/step - loss: 0.1030 - acc: 0.9580 - val_loss: 1.6941 - val_acc: 0.5298\n",
      "Epoch 14/20\n",
      "10326/10326 [==============================] - 39s 4ms/step - loss: 0.0922 - acc: 0.9648 - val_loss: 1.5934 - val_acc: 0.5568\n",
      "Epoch 15/20\n",
      "10326/10326 [==============================] - 41s 4ms/step - loss: 0.0838 - acc: 0.9675 - val_loss: 2.2137 - val_acc: 0.5363\n",
      "Epoch 16/20\n",
      "10326/10326 [==============================] - 39s 4ms/step - loss: 0.0697 - acc: 0.9754 - val_loss: 1.6377 - val_acc: 0.5663\n",
      "Epoch 17/20\n",
      "10326/10326 [==============================] - 41s 4ms/step - loss: 0.0741 - acc: 0.9717 - val_loss: 1.7847 - val_acc: 0.5528\n",
      "Epoch 18/20\n",
      "10326/10326 [==============================] - 40s 4ms/step - loss: 0.0541 - acc: 0.9805 - val_loss: 1.7468 - val_acc: 0.5448\n",
      "Epoch 19/20\n",
      "10326/10326 [==============================] - 41s 4ms/step - loss: 0.0615 - acc: 0.9770 - val_loss: 1.8750 - val_acc: 0.5663\n",
      "Epoch 20/20\n",
      "10326/10326 [==============================] - 40s 4ms/step - loss: 0.0584 - acc: 0.9794 - val_loss: 1.6607 - val_acc: 0.5533\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7eeb85bfdb38>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size=8\n",
    "lrg_model.fit(trn_features, trn_labels_1, epochs=20, batch_size=batch_size, \n",
    "       validation_data=(val_features, val_labels_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
