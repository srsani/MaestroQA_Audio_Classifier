{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audio classification through image classification:\n",
    "\n",
    "This notebook shows how to use a pre-trained resent with additional Fully Convolutional Net to predict which call is good or bad. \n",
    "\n",
    "Advantage:\n",
    "\n",
    "- The advantage of this method is that it does not require any audio file feature engineering and nor it relies on the text that is associated with each audio recording. This means it can be faster and much cheaper.\n",
    "\n",
    "<p>\n",
    "\n",
    "- An impressive 98% accuracy can be achieved with this model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cuDNN version 6021 on context None\n",
      "Mapped name None to device cuda: Quadro P6000 (0000:03:00.0)\n",
      "Using Theano backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current folder:  /home/sohrab/MaestroQA_ffast_ai\n",
      "model path:  /home/sohrab/MaestroQA_ffast_ai/Data/crop_data/models\n"
     ]
    }
   ],
   "source": [
    "# Rather than importing everything manually, we'll make things easy\n",
    "#   and load them all in utils.py, and just import them from there.\n",
    "%matplotlib inline\n",
    "import utils; \n",
    "\n",
    "import importlib\n",
    "importlib.reload(utils)\n",
    "\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "def save_array(fname, arr): \n",
    "    c=bcolz.carray(arr, rootdir=fname, mode='w'); c.flush()\n",
    "def load_array(fname):\n",
    "    return bcolz.open(fname)[:]\n",
    "\n",
    "from utils import *\n",
    "current_dir = os.getcwd()\n",
    "print ('current folder: ',current_dir)\n",
    "model_path = current_dir+'/Data/crop_data/models'\n",
    "print ('model path: ',model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import resnet50\n",
    "from resnet50 import Resnet50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading images into numpy arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4870 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "batches = image.ImageDataGenerator().flow_from_directory(current_dir+'/Data/maestroqa/train', \n",
    "                                                           target_size=(512,683),\n",
    "                                                           class_mode=None, \n",
    "                                                           shuffle=False,\n",
    "                                                           batch_size=1)\n",
    "trn_data = np.concatenate([batches.next() for i in range(batches.samples)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 800 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "val_batches = image.ImageDataGenerator().flow_from_directory(current_dir+'/Data/maestroqa/valid', \n",
    "                                                           target_size=(512,683),\n",
    "                                                           class_mode=None, \n",
    "                                                           shuffle=False,\n",
    "                                                           batch_size=1)\n",
    "val_data = np.concatenate([val_batches.next() for i in range(val_batches.samples)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4870 images belonging to 2 classes.\n",
      "Found 800 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "batches = image.ImageDataGenerator().flow_from_directory(current_dir+'/Data/maestroqa/train', \n",
    "                                                           target_size=(512,683),\n",
    "                                                           class_mode='categorical', \n",
    "                                                           shuffle=False,\n",
    "                                                           batch_size=1)\n",
    "\n",
    "val_batches = image.ImageDataGenerator().flow_from_directory(current_dir+'/Data/maestroqa/valid', \n",
    "                                                           target_size=(512,683),\n",
    "                                                           class_mode='categorical', \n",
    "                                                           shuffle=False,\n",
    "                                                           batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trn_classes = batches.classes\n",
    "val_classes = val_batches.classes\n",
    "\n",
    "trn_labels = np.array(OneHotEncoder().fit_transform(trn_classes.reshape(-1,1)).todense())\n",
    "val_labels = np.array(OneHotEncoder().fit_transform(val_classes.reshape(-1,1)).todense())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_array(current_dir+'/Data/crop_data/models/train_data_uu.bc', trn_data)\n",
    "save_array(current_dir+'/Data/crop_data/models/valid_data_uu.bc', val_data)\n",
    "\n",
    "save_array(current_dir+'/Data/crop_data/models/trn_labels_uu.bc', trn_labels)\n",
    "save_array(current_dir+'/Data/crop_data/models/val_labels_uu.bc', val_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading back the arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4870, 3, 512, 683) (800, 3, 512, 683) (4870, 2) (800, 2)\n"
     ]
    }
   ],
   "source": [
    "trn_data_1 = load_array(current_dir+'/Data/crop_data/models/train_data_uu.bc')\n",
    "val_data_1 = load_array(current_dir+'/Data/crop_data/models/valid_data_uu.bc')\n",
    "\n",
    "trn_labels_1 = load_array(current_dir+'/Data/crop_data/models/trn_labels_uu.bc')\n",
    "val_labels_1 = load_array(current_dir+'/Data/crop_data/models/val_labels_uu.bc')\n",
    "\n",
    "print (trn_data_1.shape,val_data_1.shape, trn_labels_1.shape, val_labels_1.shape )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the Resnet50 model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sohrab/anaconda3/lib/python3.5/site-packages/keras/layers/core.py:630: UserWarning: `output_shape` argument not specified for layer lambda_1 and cannot be automatically inferred with the Theano backend. Defaulting to output shape `(None, 3, 512, 683)` (same as input shape). If the expected output shape is different, specify it via the `output_shape` argument.\n",
      "  .format(self.name, input_shape))\n",
      "/home/sohrab/MaestroQA_ffast_ai/resnet50.py:61: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (7, 7), name=\"conv1\", strides=(2, 2))`\n",
      "  x = Convolution2D(64, 7, 7, subsample=(2, 2), name='conv1')(x)\n"
     ]
    }
   ],
   "source": [
    "rn0 = Resnet50(include_top=False , size= (512,683)).model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4870/4870 [==============================] - 112s 23ms/step\n",
      "800/800 [==============================] - 18s 23ms/step\n"
     ]
    }
   ],
   "source": [
    "# Extracting feature from the pre-trained model\n",
    "trn_features = rn0.predict(trn_data_1, batch_size=128, verbose=1)\n",
    "val_features = rn0.predict(val_data_1, batch_size=128, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding the Fully Convolutional Net:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nf=128; p=0.2\n",
    "def get_lrg_layers():\n",
    "    return [\n",
    "        BatchNormalization(axis=1, input_shape=(2048, 16, 22)),\n",
    "        Convolution2D(nf,3,3, activation='relu', border_mode='same'),\n",
    "        BatchNormalization(axis=1),\n",
    "        MaxPooling2D(),\n",
    "        Convolution2D(nf,3,3, activation='relu', border_mode='same'),\n",
    "        BatchNormalization(axis=1),\n",
    "         MaxPooling2D(),\n",
    "        Convolution2D(nf,3,3, activation='relu', border_mode='same'),\n",
    "        BatchNormalization(axis=1),\n",
    "        MaxPooling2D((1,1)),\n",
    "        Convolution2D(2,3,3, border_mode='same'),\n",
    "        Dropout(p),\n",
    "        GlobalAveragePooling2D(),\n",
    "        Activation('softmax')\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sohrab/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:5: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), padding=\"same\", activation=\"relu\")`\n",
      "/home/sohrab/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:8: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), padding=\"same\", activation=\"relu\")`\n",
      "/home/sohrab/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:11: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), padding=\"same\", activation=\"relu\")`\n",
      "/home/sohrab/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:14: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(2, (3, 3), padding=\"same\")`\n"
     ]
    }
   ],
   "source": [
    "lrg_model = Sequential(get_lrg_layers())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lrg_model.compile(Adam(lr=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4870 samples, validate on 800 samples\n",
      "Epoch 1/3\n",
      "4870/4870 [==============================] - 11s 2ms/step - loss: 0.4730 - acc: 0.8002 - val_loss: 0.5673 - val_acc: 0.7650\n",
      "Epoch 2/3\n",
      "4870/4870 [==============================] - 11s 2ms/step - loss: 0.2394 - acc: 0.9452 - val_loss: 0.3344 - val_acc: 0.9250\n",
      "Epoch 3/3\n",
      "4870/4870 [==============================] - 11s 2ms/step - loss: 0.1278 - acc: 0.9778 - val_loss: 0.1652 - val_acc: 0.9637\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f877ba67eb8>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K.set_value(lrg_model.optimizer.lr, 0.00001)\n",
    "batch_size=32\n",
    "lrg_model.fit(trn_features, trn_labels_1, epochs=3, batch_size=batch_size, \n",
    "       validation_data=(val_features, val_labels_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4870 samples, validate on 800 samples\n",
      "Epoch 1/3\n",
      "4870/4870 [==============================] - 10s 2ms/step - loss: 0.0702 - acc: 0.9945 - val_loss: 0.1208 - val_acc: 0.9712\n",
      "Epoch 2/3\n",
      "4870/4870 [==============================] - 10s 2ms/step - loss: 0.0517 - acc: 0.9971 - val_loss: 0.0934 - val_acc: 0.9762\n",
      "Epoch 3/3\n",
      "4870/4870 [==============================] - 10s 2ms/step - loss: 0.0401 - acc: 0.9984 - val_loss: 0.0747 - val_acc: 0.9788\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f877b2e9ba8>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K.set_value(lrg_model.optimizer.lr, 0.0001)\n",
    "batch_size=64\n",
    "lrg_model.fit(trn_features, trn_labels_1, epochs=3, batch_size=batch_size, \n",
    "       validation_data=(val_features, val_labels_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4870 samples, validate on 800 samples\n",
      "Epoch 1/3\n",
      "4870/4870 [==============================] - 10s 2ms/step - loss: 0.1085 - acc: 0.9620 - val_loss: 0.1113 - val_acc: 0.9650\n",
      "Epoch 2/3\n",
      "4870/4870 [==============================] - 10s 2ms/step - loss: 0.0343 - acc: 0.9887 - val_loss: 0.0710 - val_acc: 0.9825\n",
      "Epoch 3/3\n",
      "4870/4870 [==============================] - 10s 2ms/step - loss: 0.0106 - acc: 0.9967 - val_loss: 0.0466 - val_acc: 0.9875\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f877b2e9c88>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K.set_value(lrg_model.optimizer.lr, 0.001)\n",
    "batch_size=64\n",
    "lrg_model.fit(trn_features, trn_labels_1, epochs=3, batch_size=batch_size, \n",
    "       validation_data=(val_features, val_labels_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
